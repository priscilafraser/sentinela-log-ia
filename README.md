# SentinelaLogIA â€” DetecÃ§Ã£o Inteligente de Anomalias em Logs de Servidor

Machine Learning + SeguranÃ§a CibernÃ©tica + Streamlit Dashboard

## VisÃ£o Geral

SentinelLogAI Ã© uma ferramenta de seguranÃ§a cibernÃ©tica que utiliza Machine Learning (Isolation Forest) para identificar comportamentos anÃ´malos em logs de servidor (Apache/Nginx).
O sistema analisa cada evento, calcula um Score de Risco, classifica a severidade (CrÃ­tico/Alto/MÃ©dio/Baixo) e exibe tudo em um dashboard interativo com Streamlit.

Este projeto simula a atuaÃ§Ã£o de um analista de SOC (Security Operations Center), detectando:

- Directory Traversal

- SQL Injection (sqlmap)

- ForÃ§a bruta em /login

- Scans de paths (curl)

- Varredura de serviÃ§os (Nmap)

- Acessos suspeitos por user-agent

- Comportamentos fora do padrÃ£o normal do servidor

O resultado Ã© um mini-SIEM totalmente funcional para demonstraÃ§Ã£o.

### Arquitetura do Projeto
````bash
sentinela-log-ia/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dados_brutos_log/
â”‚   â”‚   â””â”€â”€ access.log             <-- Log de treino (normal + ataques)
â”‚   â””â”€â”€ arquivo_upload.log         <-- Log enviado pelo usuÃ¡rio no Streamlit
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sentinela_modelo.pkl       <-- Modelo treinado
â”‚   â”œâ”€â”€ sentinela_scaler.pkl       <-- Scaler (padronizaÃ§Ã£o)
â”‚   â””â”€â”€ feature_columns.json       <-- Ordem das features
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestaoLogs/
â”‚   â”‚   â”œâ”€â”€ carregar_logs.py       <-- Leitura / carregamento e extraÃ§Ã£o dos logs
â”‚   â”‚   â””â”€â”€ gerar_logs_ataque.py   <-- Gerador de logs normais + ataques
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessamento/
â”‚   â”‚   â””â”€â”€ features.py            <-- Engenharia de atributos para o modelo
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ treinamento_modelo.py      <-- Treino do Isolation Forest
â”‚   â”‚
â”‚   â””â”€â”€ dashboards/
â”‚       â””â”€â”€ app.py                 <-- Dashboard Streamlit
â”‚
â””â”€â”€ README.md
````

### Fluxo Geral â€” Como Executar o Projeto
Para rodar o SentinelaLogIA, basta seguir os passos abaixo.

1ï¸âƒ£ Preparar o ambiente

Clone o repositÃ³rio e instale as dependÃªncias:
````bash
pip install -r requirements.txt
````

2ï¸âƒ£ Preparar o arquivo de log para treino

O modelo precisa de um arquivo access.log contendo trÃ¡fego normal e/ou ataques simulados.

VocÃª pode escolher:

âœ”ï¸ OpÃ§Ã£o A â€” Usar um log real

Salve em:
````bash
data/dados_brutos_log/access.log
````

âœ”ï¸ OpÃ§Ã£o B â€” Gerar um log automaticamente

O projeto inclui um gerador de logs com ataques simulados (Traversal, SQLi, Curl Scan, Brute Force etc.):
````bash
python -m src.ingestaoLogs.gerar_logs_ataque
````

Isso gera um log completo (normal + ataques) automaticamente. E cria automaticamente o arquivo:
````bash
data/dados_brutos_log/access.log
````

3ï¸âƒ£ Treinar o modelo de Machine Learning

No terminal, na raiz do projeto, execute o script de treinamento:
````bash
python -m src.models.treinamento_modelo
````

ApÃ³s finalizar, serÃ£o gerados:
````bash
modelos/sentinela_modelo.pkl
modelos/sentinela_scaler.pkl
modelos/feature_columns.json
````

O dashboard usarÃ¡ esses arquivos para analisar novos logs enviados pelo usuÃ¡rio.

4ï¸âƒ£ Iniciar o Dashboard (Streamlit)

Agora basta rodar a interface de visualizaÃ§Ã£o:
````bash
streamlit run src/dashboards/app.py
````

A interface permite que vocÃª faÃ§a upload de qualquer arquivo .log ou .txt e receba anÃ¡lise em tempo real:

- Score de risco

- ClassificaÃ§Ã£o por severidade (CrÃ­tico/Alto/MÃ©dio/Baixo)

- Ranking ordenado de eventos suspeitos

- Cards de resumo de ataques

### Como o Score de Risco Ã© Calculado?

O Isolation Forest fornece um score interno (score_samples) onde:

valores mais negativos â†’ comportamento mais suspeito

valores prÃ³ximos de zero â†’ comportamento normal

Para facilitar a interpretaÃ§Ã£o:
````bash
score_risco = -score_samples
````

Assim:

quanto maior o score_risco, maior o risco

facilita ordenar e visualizar no dashboard

### ClassificaÃ§Ã£o de Severidade

Com base no score normalizado:
| Severidade | Score     | InterpretaÃ§Ã£o                                   |
| ---------- | --------- | ----------------------------------------------- |
| ðŸ”´ CrÃ­tico | â‰¥ 0.70    | Ataques graves (SQLi, Traversal, scans pesados) |
| ðŸŸ  Alto    | 0.66â€“0.69 | ForÃ§a bruta, user-agents suspeitos              |
| ðŸŸ¡ MÃ©dio   | 0.60â€“0.65 | Eventos fora do padrÃ£o                          |
| ðŸŸ¢ Baixo   | < 0.60    | Suspeita leve / ruÃ­do normal                    |


### Exemplos de Ataques Detectados

- /../../../../etc/passwd â†’ Directory Traversal

- /produtos?id=1 OR 1=1 â†’ SQL Injection

- POST /login repetido â†’ Brute Force

- /wp-admin / /phpmyadmin â†’ Scan de paths

- User-agent sqlmap, curl, Nmap â†’ Varredura

O modelo tambÃ©m prioriza eventos com:

- status HTTP incomuns (401,403,404,500)

- bytes fora do padrÃ£o

- URLs com entropia alta

- padrÃµes raros aparecendo repentinamente

### Dashboard Interativo

O Streamlit mostra:

âœ”ï¸ Cards com contagem por severidade

âœ”ï¸ Filtro por severidade

âœ”ï¸ Ranking ordenado por score

âœ”ï¸ PrÃ©via dos logs enviados

âœ”ï¸ Tabela de eventos suspeitos

âœ”ï¸ IP, URL, mÃ©todo, status, user-agent e score


Ã‰ literalmente um mini-SIEM funcional.

### Tecnologias Utilizadas

- Python 3.11

- Pandas

- Scikit-Learn (Isolation Forest)

- Streamlit

- Joblib

- Regex (extraÃ§Ã£o de logs)

### Status do Projeto

âœ… Modelo treinado e funcional

âœ… Dashboard pronto e interativo

âœ… Para os dados utilizados detectou 100% dos ataques simulados

âš ï¸ PrÃ³ximos passos possÃ­veis:

- adicionar grÃ¡ficos

- detecÃ§Ã£o por IP / geolocalizaÃ§Ã£o

- alertas por email

- logs em tempo real


### ðŸ§‘â€ðŸ’» Autor

Projeto desenvolvido para estudo e demonstraÃ§Ã£o de estratÃ©gias de detecÃ§Ã£o de ameaÃ§as com Machine Learning e anÃ¡lise inteligente de logs, aplicando tÃ©cnicas de DetecÃ§Ã£o de Anomalias e prÃ¡ticas de SeguranÃ§a CibernÃ©tica (Blue Team).

